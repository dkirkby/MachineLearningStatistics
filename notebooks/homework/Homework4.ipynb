{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import locate_data\n",
    "blobs_data = pd.read_hdf(locate_data('blobs_data.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement the core of the E- and M-steps for the [Gaussian mixture model (GMM)](http://scikit-learn.org/stable/modules/mixture.html) method. Note the similarities with the E- and M-steps of the K-means method.\n",
    "\n",
    "First, implement the function below to evaluate the [multidimensional Gaussian probability density](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) for arbitrary mean $\\vec{\\mu}$ and covariance matrix $C$ (refer to the lecture for more details on the notation used here):\n",
    "$$\n",
    "G(\\vec{x} ; \\vec{\\mu}, C) = \\left(2\\pi\\right)^{-D/2}\\,\\left| C\\right|^{-1/2}\\,\n",
    "\\exp\\left[  -\\frac{1}{2} \\left(\\vec{x} - \\vec{\\mu}\\right)^T C^{-1} \\left(\\vec{x} - \\vec{\\mu}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b48ba26027b4ee50d4805dba5049f685",
     "grade": false,
     "grade_id": "cell-3e1557ddc299c998",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mu, C):\n",
    "    \"\"\"Evaluate the Gaussian probability density.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        1D array of D feature values for a single sample\n",
    "    mu : array\n",
    "        1D array of D mean feature values for this component.\n",
    "    C : array\n",
    "        2D array with shape (D, D) of covariance matrix elements for this component.\n",
    "        Must be positive definite (and therefore symmetric).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Probability density.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    mu = np.asarray(mu)\n",
    "    C = np.asarray(C)\n",
    "    D = len(x)\n",
    "    assert x.shape == (D,) and mu.shape == (D,)\n",
    "    assert C.shape == (D, D)\n",
    "    assert np.allclose(C.T, C)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a5e0506b7cb3a234462031ca4aa844f1",
     "grade": true,
     "grade_id": "cell-2c49da9532bcc30e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [1], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[2]]), 1 / np.sqrt(4 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [0], [[1]]), np.exp(-0.5) / np.sqrt(2 * np.pi))\n",
    "\n",
    "assert np.allclose(Gaussian_pdf([0, 0], [0, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, -1], [1, -1], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[4, 0], [0, 1]]), 1 / (4 * np.pi))\n",
    "\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, +1], [+1, 1]]), 5) == 0.07778\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, -1], [-1, 1]]), 5) == 0.07778\n",
    "assert np.round(np.log(Gaussian_pdf([1, 0, -1], [1, 2, 3], [[4, -1, 0], [-1, 1, 0], [0, 0, 2]])), 5) == -10.31936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the E-step in the function below. This consists of calculating the relative probability that each sample $\\vec{x}_n$ ($n$-th row of $X$) belongs to each component $k$:\n",
    "$$\n",
    "p_{nk} = \\frac{\\omega_k G(\\vec{x}_n; \\vec{\\mu}_k, C_k)}\n",
    "{\\sum_{j=1}^K\\, \\omega_j G(\\vec{x}_n; \\vec{\\mu}_j, C_j)}\n",
    "$$\n",
    "Note that these relative probabilities (also called *responsibilities*) sum to one over components $k$ for each sample $n$.  Also note that we consider the parameters ($\\omega_k$, $\\vec{\\mu}_k$, $C_k$) of each component fixed during this step. *Hint: use your `Gaussian_pdf` function here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61fd366e12f666c81f70dbff0cb2112b",
     "grade": false,
     "grade_id": "cell-918f72c2d88d9e0b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def E_step(X, w, mu, C):\n",
    "    \"\"\"Perform a GMM E-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    w : array with shape (K,)\n",
    "        Per-component weights.\n",
    "    mu : array with shape (K, D)\n",
    "        Array of mean vectors for each component.\n",
    "    C : array with shape (K, D, D).\n",
    "        Array of covariance matrices for each component.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(w)\n",
    "    assert w.shape == (K,)\n",
    "    assert mu.shape == (K, D)\n",
    "    assert C.shape == (K, D, D)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea2e10e850207fd584ff9751640780ab",
     "grade": true,
     "grade_id": "cell-bdaef2ea56dd9bee",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "w = np.full(4, 0.25)\n",
    "mu = np.array([[-2], [-1], [0], [1]])\n",
    "C = np.ones((4, 1, 1))\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.258,  0.134,  0.058,  0.021,  0.006],\n",
    "     [ 0.426,  0.366,  0.258,  0.152,  0.077],\n",
    "     [ 0.258,  0.366,  0.426,  0.414,  0.346],\n",
    "     [ 0.058,  0.134,  0.258,  0.414,  0.57 ]])\n",
    "\n",
    "X = np.zeros((1, 3))\n",
    "w = np.ones((2,))\n",
    "mu = np.zeros((2, 3))\n",
    "C = np.zeros((2, 3, 3))\n",
    "diag = range(3)\n",
    "C[:, diag, diag] = 1\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.5], [ 0.5]])\n",
    "\n",
    "X = np.array([[0,0,0], [1,0,0]])\n",
    "mu = np.array([[0,0,0], [1,0,0]])\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.622,  0.378], [ 0.378,  0.622]])\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "subsample = X.reshape(K, (N//K), D)\n",
    "mu = subsample.mean(axis=1)\n",
    "C = np.empty((K, D, D))\n",
    "w = gen.uniform(size=K)\n",
    "w /= w.sum()\n",
    "for k in range(K):\n",
    "    C[k] = np.cov(subsample[k], rowvar=False)\n",
    "#print(repr(np.round(E_step(X, w, mu, C)[:, :5], 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C)[:, :5], 3) ==\n",
    "    [[ 0.422,  0.587,  0.344,  0.279,  0.19 ],\n",
    "     [ 0.234,  0.11 ,  0.269,  0.187,  0.415],\n",
    "     [ 0.291,  0.194,  0.309,  0.414,  0.279],\n",
    "     [ 0.053,  0.109,  0.077,  0.12 ,  0.116]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the M-step in the function below.  During this step, we consider the relative weights $p_{nk}$ from the previous step fixed and instead update the parameters of each component (which were fixed in the previous step), using:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\omega_k &= \\frac{1}{N}\\, \\sum_{n=1}^N\\, p_{nk} \\\\\n",
    "\\vec{\\mu}_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\vec{x}_n}{\\sum_{n=1}^N\\, p_{nk}} \\\\\n",
    "C_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\left( \\vec{x}_n - \\vec{\\mu}_k\\right) \\left( \\vec{x}_n - \\vec{\\mu}_k\\right)^T}\n",
    "{\\sum_{n=1}^N\\, p_{nk}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Make sure you understand why the last expression yields a matrix rather than a scalar dot product before jumping into the code. (If you would like a numpy challenge, try implementing this function without any loops, e.g., with `np.einsum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65d96b3fc5370a9805c19411492121a7",
     "grade": false,
     "grade_id": "cell-6cd6167c32f19845",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, p):\n",
    "    \"\"\"Perform a GMM M-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    p : array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple w, mu, C of arrays with shapes (K,), (K, D) and (K, D, D) giving\n",
    "        the updated component parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(p)\n",
    "    assert p.shape == (K, N)\n",
    "    assert np.allclose(p.sum(axis=0), 1)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afb18d41f5b985695d83f17fcbaca522",
     "grade": true,
     "grade_id": "cell-7359cf92c75ff48d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "p = np.full(20, 0.25).reshape(4, 5)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C, 5)))\n",
    "assert np.all(np.round(w, 5) == 0.25)\n",
    "assert np.all(np.round(mu, 5) == 0.0)\n",
    "assert np.all(np.round(C, 5) == 0.5)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "p = gen.uniform(size=(K, N))\n",
    "p /= p.sum(axis=0)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C[0], 5)))\n",
    "assert np.all(\n",
    "    np.round(w, 5) == [ 0.25216,  0.24961,  0.24595,  0.25229])\n",
    "assert np.all(\n",
    "    np.round(mu, 5) ==\n",
    "    [[ 0.06606,  0.06   , -0.00413,  0.01562,  0.00258],\n",
    "     [ 0.02838,  0.01299,  0.01286,  0.03068, -0.01714],\n",
    "     [ 0.03157,  0.04558, -0.01206,  0.03493, -0.0326 ],\n",
    "     [ 0.05467,  0.06293, -0.01779,  0.04454,  0.00065]])\n",
    "assert np.all(\n",
    "    np.round(C[0], 5) ==\n",
    "    [[ 0.98578,  0.01419, -0.03717,  0.01403,  0.0085 ],\n",
    "     [ 0.01419,  0.95534, -0.02724,  0.03201, -0.00648],\n",
    "     [-0.03717, -0.02724,  0.90722,  0.00313,  0.0299 ],\n",
    "     [ 0.01403,  0.03201,  0.00313,  1.02891,  0.0813 ],\n",
    "     [ 0.0085 , -0.00648,  0.0299 ,  0.0813 ,  0.922  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the GMM algorithm.  Here is a simple wrapper that uses KMeans to initialize the relative probabilities to all be either zero or one, based on each sample's cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import GMM_pairplot\n",
    "\n",
    "def GMM_fit(data, n_components, nsteps, init='random', seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    p = np.zeros((n_components, N))\n",
    "    if init == 'kmeans':\n",
    "        # Use KMeans to divide the data into clusters.\n",
    "        fit = cluster.KMeans(n_clusters=n_components, random_state=gen).fit(data)\n",
    "        # Initialize the relative weights using cluster membership.\n",
    "        # The initial weights are therefore all either 0 or 1.\n",
    "        for k in range(n_components):\n",
    "            p[k, fit.labels_ == k] = 1\n",
    "    else:\n",
    "        # Assign initial relative weights in quantiles of the first feature.\n",
    "        # This is not a good initialization strategy, but shows how well\n",
    "        # GMM converges from a poor starting point.\n",
    "        x0 = X[:, 0]\n",
    "        edges = np.percentile(x0, np.linspace(0, 100, n_components + 1))\n",
    "        for k in range(n_components):\n",
    "            quantile = (edges[k] <= x0) & (x0 <= edges[k + 1])\n",
    "            p[k, quantile] = 1.\n",
    "    # Normalize relative weights.\n",
    "    p /= p.sum(axis=0)\n",
    "    # Perform an initial M step to initialize the component params.\n",
    "    w, mu, C = M_step(X, p)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        p = E_step(X, w, mu, C)\n",
    "        w, mu, C = M_step(X, p)\n",
    "    # Plot the results.\n",
    "    GMM_pairplot(data, w, mu, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on the 3D `blobs_data` and notice that it converges close to the correct solution after 8 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0)\n",
    "GMM_fit(blobs_data, 3, nsteps=4)\n",
    "GMM_fit(blobs_data, 3, nsteps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is even faster if you use KMeans to initialize the relative weights (which is why most implementations do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0, init='kmeans')\n",
    "GMM_fit(blobs_data, 3, nsteps=1, init='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A density estimator should provide a probability density function $P(\\vec{x})$ that is normalized over its feature space $\\vec{x}$\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) = 1 \\; .\n",
    "$$\n",
    "In this problem you will verify this normalization for KDE using two different numerical approaches for the integral.\n",
    "\n",
    "First, implement the function below to accept a 1D KDE fit object and estimate its normalization integral using the trapezoid rule with the specified grid. *Hint: the `np.trapz` function will be useful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7c730306185db01f0a5f377b61660a4",
     "grade": false,
     "grade_id": "cell-f08e5071751cdabd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_grid_normalization(fit, xlo, xhi, ngrid):\n",
    "    \"\"\"Check 1D denstity estimator fit result normlization using grid quadrature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.kde.KernelDensity fit object\n",
    "        Result of fit to 1D dataset.\n",
    "    xlo : float\n",
    "        Low edge of 1D integration range.\n",
    "    xhi : float\n",
    "        High edge of 1D integration range.\n",
    "    ngrid : int\n",
    "        Number of equally spaced grid points covering [xlo, xhi],\n",
    "        including both end points.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ec1ffc451d8c87dc51acb674601e07b",
     "grade": true,
     "grade_id": "cell-d535958d6d6e7dc6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']))\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 5), 3) == 1.351\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 10), 3) == 1.019\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 20), 3) == 0.986\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x2']))\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 5), 3) == 1.108\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 10), 3) == 0.993\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 20), 3) == 0.971\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x1']))\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 5), 3) == 1.311\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 10), 3) == 0.954\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 20), 3) == 1.028\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 100), 3) == 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to estimate a multidimensional fit normalization using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration):\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) \\simeq \\frac{V}{N_{mc}}\\, \\sum_{j=1}^{N_{mc}} P(\\vec{x}_j) = V \\langle P\\rangle \\; ,\n",
    "$$\n",
    "where the $\\vec{x}_j$ are uniformly distributed over the integration domain and $V$ is the integration domain volume. Note that `trapz` gives more accurate results for a fixed number of $P(\\vec{x})$ evaluations, but MC integration is much easier to generalize to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f557f4e4f8cc2c1e6944872cb1b3448e",
     "grade": false,
     "grade_id": "cell-6a773ed33ecb1799",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_mc_normalization(fit, xlo, xhi, nmc, seed=123):\n",
    "    \"\"\"Check denstity estimator fit result normlization using MC integration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.kde.KernelDensity fit object\n",
    "        Result of fit to arbitrary dataset of dimension D.\n",
    "    xlo : array\n",
    "        1D array of length D with low limits of integration domain along each dimension.\n",
    "    xhi : array\n",
    "        1D array of length D with high limits of integration domain along each dimension.\n",
    "    nmc : int\n",
    "        Number of random MC integration points within the domain to use.\n",
    "    \"\"\"\n",
    "    xlo = np.asarray(xlo)\n",
    "    xhi = np.asarray(xhi)\n",
    "    assert xlo.shape == xhi.shape\n",
    "    assert np.all(xhi > xlo)\n",
    "    D = len(xlo)\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "276d5e5fe9b26b0e892d8f1327a6fe22",
     "grade": true,
     "grade_id": "cell-7b392286cb763415",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "##### A correct solution should pass these tests.\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']))\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10), 3) == 1.129\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 100), 3) == 1.022\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 1000), 3) == 1.010\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10000), 3) == 0.999\n",
    "\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x2']))\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10), 3) == 1.754\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 100), 3) == 1.393\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 1000), 3) == 0.924\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10000), 3) == 1.019\n",
    "\n",
    "fit = neighbors.kde.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10), 3) == 2.797\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 100), 3) == 0.613\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 1000), 3) == 1.316\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10000), 3) == 1.139"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
