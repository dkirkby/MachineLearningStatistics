{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have an algorithm to identify [quasars](https://en.wikipedia.org/wiki/Quasar) from astronomical images, which simply returns `True` or `False`. Using control samples, you have determined that your algorithm has the following performance for real quasars as well as the main contaminants for a quasar sample: galaxies and stars.\n",
    "\n",
    "| data? | M=quasar | M=galaxy | M=star |\n",
    "|---|--------|--------|------|\n",
    "| D=True  |  0.8   |   0.1  |  0.2 |\n",
    "| D=False |  0.2   |   0.9  |  0.8 |\n",
    "\n",
    "Implement the following function to calculate the likelihood $P(D\\mid M)$ given this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "059afe0d6e06bdbb12fb30256e509f38",
     "grade": false,
     "grade_id": "cell-2e9a416384cd060f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def likelihood(D, M):\n",
    "    \"\"\"Calculate likelihood of data D given model M.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : bool\n",
    "        A boolean (True/False) value indicating whether the algorithm identified an\n",
    "        object as being a quasar or not.\n",
    "    M : str\n",
    "        A string ('quasar', 'galaxy', 'star') specifying the assumed model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The probability of the data given the model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a8a1cb3cb82392f43df1423711091b3f",
     "grade": true,
     "grade_id": "cell-8552b070717bc898",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert likelihood(True, 'quasar') == 0.8\n",
    "assert likelihood(False, 'galaxy') == 0.9\n",
    "assert likelihood(True, 'star') == 0.2\n",
    "for M in 'quasar', 'galaxy', 'star':\n",
    "    assert likelihood(True, M) + likelihood(False, M) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability of each model will vary between images, depending mostly on the local density of stars which can vary a lot (and is especially high when you look through the disk of the Milky Way).\n",
    "\n",
    "Implement the function below to calculate the prior probabilities of each model for an image based on the expected number of objects of each type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e02bb19563b72415966eda79af339b33",
     "grade": false,
     "grade_id": "cell-bf516813ae33cf9e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def prior(num_quasars_expected, num_galaxies_expected, num_stars_expected):\n",
    "    \"\"\"Calculate the prior probability of each model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_quasars_expected : int\n",
    "        Number of expected quasars.\n",
    "    num_galaxies_expected : int\n",
    "        Number of expected galaxies.\n",
    "    num_stars_expected : int\n",
    "        Number of expected stars.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of prior probabilities for each model with keys 'quasar',\n",
    "        'galaxy' and 'star'.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08f0689c5fb627efa6fa4eb338bd4e2f",
     "grade": true,
     "grade_id": "cell-67ac4a99d1ac73df",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert prior(100, 100, 200) == {'quasar': 0.25, 'galaxy': 0.25, 'star': 0.5}\n",
    "assert prior(100, 100, 600) == {'quasar': 0.125, 'galaxy': 0.125, 'star': 0.75}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now the code necessary to quantify how well your quasar identification algorithm performs in regions with different densities of stars, using the posterior probability $P(\\text{quasar}\\mid \\text{True})$.  For example, if the stellar density increases 3 times (from 200 to 600 per image) with fixed quasar and galaxy densities (100 each), the posterior probability drops from 0.615 to 0.381:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learn(prior(100, 100, 200), likelihood, True)\n",
    "Learn(prior(100, 100, 600), likelihood, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you measure a random process that follows an [exponential decay law](https://en.wikipedia.org/wiki/Exponential_decay) for the number $n(t)$ of un-decayed states as a function of time $t$:\n",
    "$$\n",
    "\\frac{dn}{dt} = -\\lambda n \\; ,\n",
    "$$\n",
    "and want to infer the posterior probability of the decay rate $\\lambda$ given your data.\n",
    "\n",
    "First, implement the function below to evaluate the likelihood of observing $N$ decay times $D = \\{t_1, t_2, \\ldots\\}$ as:\n",
    "$$\n",
    "P(D\\mid \\lambda) = \\prod_{i=1}^{N}\\, P(t_i\\mid \\lambda)\n",
    "$$\n",
    "where the **un-normalized** probability density for exponential decay is:\n",
    "$$\n",
    "P(t\\mid \\lambda) \\propto \\exp(-\\lambda t) \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ff86a13653a206d1c0d3288ebd7bec1b",
     "grade": false,
     "grade_id": "cell-c0dbc24f1cd97593",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decay_likelihood(decay_times, lam):\n",
    "    \"\"\"Calculate the normalized likelihood of measured times assuming a decay rate.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93a5e21b707da578410ef8c42296aad0",
     "grade": true,
     "grade_id": "cell-6920ac5d023a6196",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert decay_likelihood([], 1) == 1\n",
    "assert np.round(decay_likelihood([1], 0.1), 4) == 0.0905\n",
    "assert np.round(decay_likelihood([1], 1.0), 4) == 0.3679\n",
    "assert np.round(decay_likelihood([1], 1.5), 4) == 0.3347\n",
    "assert np.round(decay_likelihood([1,2,1], 0.1), 4) == 0.0007\n",
    "assert np.round(decay_likelihood([1,2,1], 1.0), 4) == 0.0183\n",
    "assert np.round(decay_likelihood([1,2,1], 1.5), 4) == 0.0084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our prior, we use the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution), which has two hyperparameters $\\alpha$ and $\\beta$:\n",
    "$$\n",
    "P(\\lambda\\mid \\alpha,\\beta) = \\frac{\\beta^\\alpha \\lambda^{\\alpha-1} e^{-\\beta\\lambda}}{\\Gamma(\\alpha)} \\; .\n",
    "$$\n",
    "Implement the function below to evaluate the Gamma distribtion PDF using a numpy expression for the numerator and [scipy.special.gamma](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.gamma.html) for the denominator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdf3e5c710533907295a8a1fc19b2cfc",
     "grade": false,
     "grade_id": "cell-e4d295f08943b3c7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def gamma_distribution(lam, alpha, beta):\n",
    "    \"\"\"Evaluate the gamma distribution.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4f1bc25ccd537ba8c3869dcf579356a1",
     "grade": true,
     "grade_id": "cell-9ad8e44e75e3939c",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "import scipy.stats\n",
    "assert gamma_distribution(1, 0, 1) == 0\n",
    "assert gamma_distribution(1, 1, 0) == 0\n",
    "for lam in (0.1, 1, 2):\n",
    "    for alpha, beta in (1, 1), (2, 1), (2, 2):\n",
    "        assert np.allclose(\n",
    "            gamma_distribution(lam, alpha, beta),\n",
    "            scipy.stats.gamma.pdf(lam, a=alpha, scale=1/beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this choice of prior is that the evidence integral can be performed analytically:\n",
    "$$\n",
    "P(D\\mid \\alpha,\\beta) = \\int d\\lambda\\, P(D\\mid\\lambda)\\, P(P(\\lambda\\mid \\alpha,\\beta)\n",
    "= \\frac{\\beta^\\alpha (\\beta + T)^{-(\\alpha+N)} \\Gamma(\\alpha+N)}{\\Gamma(\\alpha)} \\; .\n",
    "$$\n",
    "Use this result to convince yourself that the posterior $P(\\lambda\\mid D,\\alpha,\\beta)$ is another Gamma distribution, but with different hyperparameter values $\\alpha'$ and $\\beta'$.  Priors and posteriors with the same functional form for some likelihood are called [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior). The `binomial_learn` example in class also used conjugate priors.\n",
    "\n",
    "Implement the function below to learn from measured decay times by updating the prior hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "858043a24a5fb6174a628153035913cd",
     "grade": false,
     "grade_id": "cell-eaddca141d275ef6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rate_learn(prior_alpha, prior_beta, decay_times):\n",
    "    \"\"\"Learn from data to update hyperparameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prior_alpha : float\n",
    "        Hyperparameter alpha for the prior Gamma distribution PDF.\n",
    "    prior_beta : float\n",
    "        Hyperparameter beta for the prior Gamma distribution PDF.\n",
    "    decay_times : array\n",
    "        Array of observed decay times.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (post_alpha, post_beta) of hyperparameter values for the\n",
    "        posterior Gamma distribution PDF.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85277587a37432b86184983c455e6ca4",
     "grade": true,
     "grade_id": "cell-cd3655417a5e4431",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert rate_learn(1, 1, []) == (1, 1)\n",
    "assert rate_learn(2, 1, []) == (2, 1)\n",
    "assert rate_learn(1, 2, []) == (1, 2)\n",
    "assert np.allclose(\n",
    "    np.round(rate_learn(np.sqrt(10), np.pi, [1,2,1]), 3),\n",
    "    (6.162, 7.142))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function below to test your code visually and confirm that better data reduces the influence of the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_learn_plot(prior_alpha, prior_beta, num_decays, true_lam, seed=123):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Generate some random decays using the true decay rate.\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    decay_times = scipy.stats.expon.rvs(scale=1 / true_lam, size=num_decays, random_state=gen)\n",
    "    \n",
    "    # Use Bayes' rule to learn from the data.\n",
    "    lam = np.linspace(0., 2.5 * true_lam, 250)\n",
    "    prior = gamma_distribution(lam, prior_alpha, prior_beta)\n",
    "    like = decay_likelihood(decay_times, lam)\n",
    "    post_alpha, post_beta = rate_learn(prior_alpha, prior_beta, decay_times)\n",
    "    post = gamma_distribution(lam, post_alpha, post_beta)\n",
    "    \n",
    "    # Plot a summary of the learning process.\n",
    "    plt.fill_between(lam, prior, alpha=0.25)\n",
    "    plt.plot(lam, prior, label='Prior')\n",
    "    plt.plot(lam, like / np.max(like) * np.max(prior), 'k:', label='Likelihood')\n",
    "    plt.fill_between(lam, post, alpha=0.25)\n",
    "    plt.plot(lam, post, label='Posterior')\n",
    "    plt.axvline(true_lam, c='r', ls='--')\n",
    "    plt.xlabel('Decay rate $\\lambda$')\n",
    "    plt.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlim(0, lam[-1])\n",
    "    plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_learn_plot(prior_alpha=1, prior_beta=0.2, num_decays=10, true_lam=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_learn_plot(prior_alpha=1, prior_beta=0.2, num_decays=100, true_lam=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will solve the same decay rate inference problem but this time using a numerical estimate based on Markov-chain Monte Carlo (MCMC).\n",
    "\n",
    "Recall that `MCMC_sample()` generates samples using a function proportional to the desired PDF. Implement the function below to evaluate the logarithm of the un-normalized posterior probability density:\n",
    "$$\n",
    "\\log P(D\\mid \\lambda) + \\log P(\\lambda\\mid \\alpha, \\beta) \\; .\n",
    "$$\n",
    "Do not call your `decay_likelihood()` or `gamma_distribution()` functions in your implementation since the result has better accuracy if you apply the logarithm and simplify analytically. Since MCMC sampling only requires a function proportional to the desired PDF, you can drop any factors in $P(D\\mid \\lambda)$ or $P(\\lambda\\mid \\alpha, \\beta)$ that do not depend on $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8fbb4b59d655403c2b88afa7c4d2601",
     "grade": false,
     "grade_id": "cell-3dc153e6b399100c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def decay_logf(lam, decay_times, prior_alpha, prior_beta):\n",
    "    \"\"\"Evaluate a function proportional to the log-posterior probability density.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lam : float\n",
    "        Decay rate parameter.\n",
    "    decay_times : array\n",
    "        Array of observed decay times.\n",
    "    prior_alpha : float\n",
    "        Hyperparameter alpha for the prior Gamma distribution PDF.\n",
    "    prior_beta : float\n",
    "        Hyperparameter beta for the prior Gamma distribution PDF.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        log P(D|lam) + log P(lam|alpha,beta) up to a constant that does not\n",
    "        depend on the value of lam. Returns -np.inf when lam <= 0.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "038631c14de25e1c6e5045e61eb74fa6",
     "grade": true,
     "grade_id": "cell-4c5f25379db996d1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "a, b = 1, 0.2\n",
    "offset = decay_logf(1, [], a, b)\n",
    "assert np.allclose(decay_logf(0.1, [], a, b) - offset, +0.18)\n",
    "assert np.allclose(decay_logf(2, [], a, b) - offset, -0.2)\n",
    "assert np.allclose(np.round(decay_logf(0.1, [1,2,1], a, b), 3) - offset, -7.128)\n",
    "assert np.allclose(np.round(decay_logf(1, [1,2,1], a, b), 3) - offset, -4.000)\n",
    "assert np.allclose(np.round(decay_logf(2, [1,2,1], a, b), 3) - offset, -6.121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function below to test your numerical solution and compare with the posterior found using `rate_learn_plot` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import MCMC_sample\n",
    "\n",
    "def MCMC_rate_learn_plot(prior_alpha, prior_beta, num_decays, true_lam, seed=123):\n",
    "    # Generate some random decays using the true decay rate.\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    decay_times = scipy.stats.expon.rvs(scale=1 / true_lam, size=num_decays, random_state=gen)\n",
    "    # Generate MCMC samples of the decay rate for this data with this prior.\n",
    "    samples = MCMC_sample(decay_logf, lam=[true_lam], decay_times=decay_times,\n",
    "                          prior_alpha=prior_alpha, prior_beta=prior_beta, nsamples=20000)\n",
    "    # Plot samples.\n",
    "    plt.hist(samples['lam'], range=(0, 2.5 * true_lam), bins=40, normed=True)\n",
    "    plt.axvline(true_lam, c='r', ls='--')\n",
    "    plt.xlabel('Decay rate $\\lambda$')\n",
    "    plt.xlim(0, 2.5 * true_lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_rate_learn_plot(prior_alpha=1, prior_beta=0.2, num_decays=10, true_lam=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_rate_learn_plot(prior_alpha=1, prior_beta=0.2, num_decays=100, true_lam=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
