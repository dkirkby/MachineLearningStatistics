{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as anp\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import locate_data, cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default score function used by sklearn to evaluate how well a regression model predicts data is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$. Implement the function below to calculate $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "454c4875daf4ea02b91678ba27b353b6",
     "grade": false,
     "grade_id": "cell-11235479a624a1eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_R2(y_data, y_pred):\n",
    "    \"\"\"Calculate the coefficient of determination R2 for two arrays.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_data : array\n",
    "        Array of data values, must have the same shape as y_pred.\n",
    "    y_pred : array\n",
    "        Array of predicted values, must have the same shape as y_data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Calculated coefficient of determination R2.\n",
    "    \"\"\"\n",
    "    assert y_data.shape == y_pred.shape\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ef649b89da55533637b33997831edfd",
     "grade": true,
     "grade_id": "cell-98a3e782013f56f8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "N = 100\n",
    "x = gen.uniform(size=N)\n",
    "y_pred = 2 * x - 1\n",
    "y_data = y_pred + gen.normal(scale=0.1, size=N)\n",
    "assert np.round(calculate_R2(y_data, y_pred), 3) == 0.961\n",
    "assert np.round(calculate_R2(y_data, -y_pred), 3) == -2.935\n",
    "assert np.round(calculate_R2(y_pred, y_pred), 3) == 1.000\n",
    "assert np.round(calculate_R2(y_pred, -y_pred), 3) == -3.000\n",
    "assert np.round(calculate_R2(y_data, np.full(N, np.mean(y_pred))), 3) == 0.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to perform a [grid-search cross validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) of a [random forest regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) over the following grid:\n",
    " - `min_samples_leaf` = 1, 10, 20\n",
    " - `n_estimators` = 5, 10, 15\n",
    " \n",
    "Hint: you will need to ensure reproducible \"random\" behavior in order to pass all the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cb096916d1e9b0fccb71fd93b43b1652",
     "grade": false,
     "grade_id": "cell-35b56d4c6f0830d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate(X, Y, gen):\n",
    "    \"\"\"Perform cross validation of a random forest regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        Array with shape (N, DX) of N samples with DX features.\n",
    "    Y : array\n",
    "        Array with shape (N, DY) of N samples with DY features.\n",
    "    gen : np.random.RandomState\n",
    "        Random state to use for reproducible random numbers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Cross-validation summary table produced by cv_summary().\n",
    "    \"\"\"\n",
    "    assert len(X) == len(Y)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ab0e9f4742f63eefe211f24d881963a",
     "grade": false,
     "grade_id": "cell-3d5234f36e9bd587",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_hdf(locate_data('spectra_data.hf5')).values\n",
    "Y = pd.read_hdf(locate_data('spectra_targets.hf5')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1753176e30f44c6a0c5852a3e72f4185",
     "grade": true,
     "grade_id": "cell-b1f9c4e7840ec102",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "cvs = cross_validate(X, Y, gen)\n",
    "assert np.all(cvs.columns.values == [\n",
    "    'mean_test', 'mean_train', 'min_samples_leaf', 'n_estimators', 'split0_test',\n",
    "    'split0_train', 'split1_test', 'split1_train', 'std_test', 'std_train'])\n",
    "assert np.all(cvs['min_samples_leaf'].values == [1, 1, 1, 10, 10, 10, 20, 20, 20])\n",
    "assert np.all(cvs['n_estimators'].values == [15, 10, 5, 15, 10, 5, 15, 10, 5])\n",
    "assert np.allclose(\n",
    "    cvs['mean_test'].values,\n",
    "    [0.962, 0.957, 0.944, 0.907, 0.904, 0.891, 0.610, 0.605, 0.593], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    cvs['mean_train'].values,\n",
    "    [0.994, 0.993, 0.991, 0.920, 0.920, 0.917, 0.621, 0.624, 0.615], atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we hand-tuned a network to assign 2D points to the correct cluster, then repeated the exercise using tensorflow to automatically learn network parameters from the training data. In this problem, you will construct and optimize the same neural network by hand (without tensorflow) to get a deeper understanding of what is involved (and why tensorflow saves a lot of work).\n",
    "\n",
    "First, load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(locate_data('circles_data.hf5')).values\n",
    "y = pd.read_hdf(locate_data('circles_targets.hf5')).values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circles():\n",
    "    cmap = sns.color_palette('colorblind', 2)\n",
    "    colors = [cmap[int(c)] for c in y]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "    plt.gca().set_aspect(1)\n",
    "    \n",
    "plot_circles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to evalute the network prediction given its parameter values and a 2D input point $(x_1, x_2)$. Recall that the network consists of:\n",
    " - 2 input nodes\n",
    " - 4 hidden nodes with sigmoid activation\n",
    " - 1 output node with sigmoid activation\n",
    " \n",
    "The corresponding mathematical function is:\n",
    "$$\n",
    "\\begin{align}\n",
    "y^{out}(\\mathbf{x},\\Theta)\n",
    "&= \\phi\\left([u_1, u_2, u_3, u_4]\\cdot \\mathbf{G}(x_1, x_2) + a \\right) \\\\\n",
    "\\mathbf{G}(x_1, x_2) &= \\phi\\left(\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} \\\\\n",
    "v_{21} & v_{22} \\\\\n",
    "v_{31} & v_{32} \\\\\n",
    "v_{41} & v_{42}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} +\n",
    "\\begin{bmatrix}b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4\\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align} \\\\\n",
    "\\phi(s) = \\frac{1}{1 + e^{-s}}\n",
    "$$\n",
    "In the function, we pack all the parameters into a single array to simplify the later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1f3772e56362fe857be81be91d307809",
     "grade": false,
     "grade_id": "cell-0106a1c071acfb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def network(x, params):\n",
    "    \"\"\"Evaluate the network output at (x1,x2) with the specified parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        Array of length 2 giving the 2D coordinates of a point to classify.\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the network's single output node.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff4b71d78b412101390af3f3d5d38493",
     "grade": true,
     "grade_id": "cell-a73df46e7a2172d5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "initial = np.array([\n",
    "    5,5,5,5,              # ui\n",
    "    -18,                  # a\n",
    "    8,0, -8,0, 0,8, 0,-8, # vij\n",
    "    10,10,10,10           # bi\n",
    "], dtype=float)\n",
    "assert np.round(network([0, 0], initial), 3) == 0.881\n",
    "assert np.round(network([0, 1], initial), 3) == 0.803\n",
    "assert np.round(network([1, 0], initial), 3) == 0.803\n",
    "assert np.round(network([1, -1], initial), 3) == 0.692\n",
    "assert np.round(network([-1, 1], initial), 3) == 0.692"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn the parameters from the data by optimizing the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss function:\n",
    "$$\n",
    "E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) =\n",
    "-\\sum_{i=1}^N\\, \\left[\n",
    "y^{train}_i \\log y^{out}(\\mathbf{X}^{train}_i, \\Theta) + (1 - y^{train}_i) \\log (1 - y^{out}(\\mathbf{X}^{train}_i, \\Theta)) \\right]\n",
    "$$\n",
    "(Refer to the Neural Network lecture notes for details on why this is a good loss function).\n",
    "\n",
    "Implement the function below to evaluate this loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2069e9143177b92143616114827ea408",
     "grade": false,
     "grade_id": "cell-1c3b6059f054ac65",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(params, X_train, y_train):\n",
    "    \"\"\"Evaluate the cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "    X_train : array\n",
    "        Array of shape (N, 2) with training values.\n",
    "    y_train : array\n",
    "        Array of N target values which are each either 0. or 1.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b78a924ce35ac9ec846c879767a527d3",
     "grade": true,
     "grade_id": "cell-7cac2e52358561d7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "assert np.round(loss(initial, np.array([[0.,0.]]), np.array([0.])), 3) == 2.126\n",
    "assert np.round(loss(initial, np.array([[0.,0.]]), np.array([1.])), 3) == 0.127\n",
    "assert np.round(loss(initial, np.array([[2.,1.]]), np.array([0.])), 3) == 0.027\n",
    "assert np.round(loss(initial, np.array([[2.,1.]]), np.array([1.])), 3) == 3.611\n",
    "assert np.round(loss(initial, X, y), 1) == 41.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the initial parameter values are at a local minimum of the loss by running 1D scans along each parameter axis. Scans are also useful for rough numerical estimates of the partial derivatives\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_k} E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scans(params, X_train, y_train, step_size=0.5, n=3):\n",
    "    names = 'u1,u2,u3,u4,a,v11,v12,v21,v22,v31,v32,v41,v42,b1,b2,b3,b4'.split(',')\n",
    "    dp_grid = np.linspace(-step_size, +step_size, 2 * n + 1)\n",
    "    L = np.empty_like(dp_grid)\n",
    "    for i, p0 in enumerate(initial):\n",
    "        pscan = params.copy()\n",
    "        for j, dp in enumerate(dp_grid):\n",
    "            pscan[i] = p0 + dp\n",
    "            L[j] = loss(pscan, X_train, y_train)\n",
    "        plt.plot(dp_grid, L, label=names[i])\n",
    "        print('numerical grad[{}] = {:.3f}'.format(names[i], np.gradient(L, dp_grid)[n]))\n",
    "    plt.legend(ncol=5)\n",
    "    plt.ylim(35., 50.)\n",
    "    plt.xlabel('Change in parameter value')\n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "\n",
    "plot_scans(initial, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the function below to calculate the partial derivatives of the loss function,\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Theta_k} E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) \\; .\n",
    "$$\n",
    "These should be calculated to high accuracy, to allow stable optimization, so either using the autograd package for automatic differentiation (refer to the Optimization lecture) or else using derivative formulas that you calculate by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bf615f13c8da77e08c7b216b8f4767d2",
     "grade": false,
     "grade_id": "cell-dac6169cbccf26d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_gradient(params, X_train, y_train):\n",
    "    \"\"\"Calculate the partial derivatives of the loss function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "    X_train : array\n",
    "        Array of shape (N, 2) with training values.\n",
    "    y_train : array\n",
    "        Array of N target values which are each either 0. or 1.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Array of length 17 containing the partial derivatives of the loss function\n",
    "        with respect to each of the parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c68468f3143cd64cdbe2b056f933eca",
     "grade": true,
     "grade_id": "cell-fd2f9cce7531b7ac",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[0.,0.]]), np.array([0.])), 3),\n",
    "    [ 0.881, 0.881, 0.881, 0.881, 0.881, 0., 0., 0.,\n",
    "      0., 0., 0., 0., 0., 0., 0., 0., 0.], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[0.,0.]]), np.array([1.])), 3),\n",
    "    [ -0.119, -0.119, -0.119, -0.119, -0.119, 0., 0.,\n",
    "      0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[2.,1.]]), np.array([0.])), 3),\n",
    "    [ 0.027, 0., 0.027, 0.024, 0.027, 0., 0., 0.001,\n",
    "      0., 0., 0., 0.028, 0.014, 0., 0., 0., 0.014], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[2.,1.]]), np.array([1.])), 3),\n",
    "    [ -0.973, -0.002, -0.973, -0.857, -0.973, -0., -0., -0.024,\n",
    "      -0.012, 0., 0., -1.022, -0.511, 0., -0.012, 0., -0.511], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, X, y), 1),\n",
    "    [ -24.1, -24.1, -24.1, -24.1, -21.9, -0.5, 0.0, 0.5, 0.0,\n",
    "      0.0, -0.5, 0.0, 0.4, 0.3, 0.3, 0.3, 0.3 ], atol=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented all of the pieces necessary to optimize the 17 network parameters using the gradient descent update rule,\n",
    "$$\n",
    "\\Theta \\rightarrow \\Theta - \\eta \\nabla E(\\Theta) \\; ,\n",
    "$$\n",
    "where $\\eta$ is the learning rate.  The large values of some of the derivatives we found above indicate that $\\eta$ will need to be small in order for the optimization to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(initial, X_train, y_train, eta=0.001, n_steps=20):\n",
    "    loss_history = [loss(initial, X_train, y_train)]\n",
    "    params = initial.copy()\n",
    "    for i in range(n_steps):\n",
    "        params -= eta * loss_gradient(params, X_train, y_train)\n",
    "        loss_history.append(loss(params, X_train, y_train))\n",
    "        print('step {:02d}: loss={:.1f}'.format(i + 1, loss_history[-1]))\n",
    "    plt.plot(loss_history, 'o-')\n",
    "    plt.xlabel('Optimization step')\n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "\n",
    "optimize(initial, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
