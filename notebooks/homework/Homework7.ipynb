{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Gaussian Process](https://en.wikipedia.org/wiki/Gaussian_process) specifies a probability density over functions $f(x)$, instead of random variables. For practical work, we usually only work with the values of Gaussian Process (GP) functions $f(x_i)$ at $N$ specified values of $x = x_i$. By definition, probability density of the resulting $f(x_i)$ values is a multivariate Gaussian with $N\\times N$ covariance matrix:\n",
    "$$\n",
    "C_{ij} = K(x_i, x_j) \\; ,\n",
    "$$\n",
    "and the mean of each $f(x_i)$ is usually taken to be zero.  A popular choice for $K$ is the \"squared-exponential covariance\":\n",
    "$$\n",
    "K(x, x') = \\sigma_0^2 \\exp\\left[ -\\frac{1}{2}\\left( \\frac{x - x'}{\\lambda}\\right)^2\\right] \\; ,\n",
    "$$\n",
    "which has two hyperparameters, $\\sigma_0$ and $\\lambda$. This choice is *stationary*, i.e., translation invariant since $K(x + x_0, x' + x_0) = K(x, x')$. Although this choice of $K$ is itself a Gaussian function, non Gaussian functions are also valid, and $K$ is not the \"Gaussian\" referred to in the GP name.\n",
    "\n",
    "To sample functions $f(x)$ from a GP at $N$ specified values of $x_i$, you can simply build the $N\\times N$ covariance matrix $C$, then generate values of $f(x_i)$ using [standard methods](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.multivariate_normal.html). Implement the function below using this approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d2cfd40db825b968144e0532c8abdbf8",
     "grade": false,
     "grade_id": "cell-79b54173f8b574ad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gpsample(x, sig0, lam, ngen, gen=None):\n",
    "    \"\"\"Generate samples f(x) of functions drawn from a Gaussian process.\n",
    "    \n",
    "    The Gaussian process has zero mean and uses the squared-exponential covariance:\n",
    "    \n",
    "      K(x1,x2) = sig0 ** 2 exp[ -0.5 * ((x1 - x2) / lam) ** 2]\n",
    "      \n",
    "    with hyperparameters sig0 and lam.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        1D array of N values of x where each sampled f(x) will be evaluated.\n",
    "    sig0 : float\n",
    "        Gaussian process hyperparameter specifying the vertical scale.\n",
    "    lam : float\n",
    "        Gaussian process hyperparameter specifying the correlation length in x.\n",
    "    ngen : int\n",
    "        Number of functions f(x) to generate with sampling.\n",
    "    gen : np.random.RandomState, int or None\n",
    "        Generator or seed to use for reproducible random numbers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        2D array with shape (ngen, N) of ngen sampled functions f(x), each\n",
    "        evaluated at the N input x coordinates.\n",
    "    \"\"\"\n",
    "    if not isinstance(gen, np.random.RandomState):\n",
    "        gen = np.random.RandomState(gen)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac97b83347889a14b1e122e80eb02927",
     "grade": true,
     "grade_id": "cell-4b9d7eed563290ad",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "x = np.linspace(-1., 1., 64)\n",
    "gen = np.random.RandomState(seed=123)\n",
    "fx = gpsample(x, sig0=1.0, lam=1.0, ngen=10000, gen=gen)\n",
    "assert fx.shape == (10000, 64)\n",
    "assert np.round(np.mean(fx), 1) == 0.0\n",
    "assert np.round(np.std(fx), 1) == 1.0\n",
    "assert np.round(np.cov(fx[:, 0], fx[:, -1])[0, 1], 1) == 0.1\n",
    "fx = gpsample(x, sig0=1.0, lam=0.5, ngen=10000, gen=gen)\n",
    "assert np.round(np.mean(fx), 1) == 0.0\n",
    "assert np.round(np.std(fx), 1) == 1.0\n",
    "assert np.round(np.cov(fx[:, 0], fx[:, -1])[0, 1], 1) == 0.0\n",
    "fx = gpsample(x, sig0=2.0, lam=2.0, ngen=10000, gen=gen)\n",
    "assert np.round(np.mean(fx), 1) == 0.0\n",
    "assert np.round(np.std(fx), 1) == 2.0\n",
    "assert np.round(np.cov(fx[:, 0], fx[:, -1])[0, 1], 1) == 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a few functions sampled from a Gaussian process using different values of the hyperparameters and observe their influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42642b3d5af8728a23718206ff1d7a98",
     "grade": false,
     "grade_id": "cell-c541207ff3883105",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1., 1., 100)\n",
    "gen = np.random.RandomState(seed=123)\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(9, 6))\n",
    "for sig0, lam, ax in zip((1.0, 1.0, 2.0, 2.0), (1.0, 0.25, 1.0, 0.25), axes.flatten()):\n",
    "    fx = gpsample(x, sig0, lam, ngen=10, gen=gen)\n",
    "    ax.plot(x, fx.T, '-')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$f(x)$')\n",
    "    ax.text(0.1, 0.9, f'$\\\\sigma_0={sig0:.1f},\\\\ \\\\lambda={lam:.2f}$',\n",
    "            transform=ax.transAxes, fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian Process is a good prior for learning a function that explains some observed data. A particularly nice feature for scientific applications is that GP learning can use the uncertainties in the observed data and provides a fully Bayesian posterior probability density.\n",
    "\n",
    "Samples of the Bayesian posterior can be obtained by randomly selecting samples of the prior with probability equal to their likelihood. If we have observed $f_\\text{obs}$ at $x_\\text{obs}$ with uncertainty $\\delta f_\\text{obs}$, then the likelihood of a function $f(x)$ is the normal probability density:\n",
    "$$\n",
    "{\\cal N}\\left(x = f_\\text{obs} - f(x_\\text{obs}) \\mid \\mu=0, \\sigma=\\delta f_\\text{obs}\\right)\n",
    "$$\n",
    "With observations at multiple values of $x$, the combined likelihood is the product of factors like this (which can be calculated more accurately using the sum of log likelihoods).\n",
    "\n",
    "Implement the function below to \"learn\" a Gaussian Process that explains some observed data using this approach. Use your `gpsample` to sample from the prior and [scipy.stats.norm.logpdf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) to evaluate the necessary log-likelihood values. Hint: since you will be randomly selecting some fraction of the prior samples you will need some iteration in order to return exactly `ngen` posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a72ef418eefd1f6055bb77435da7d56e",
     "grade": false,
     "grade_id": "cell-26aa9c9981071eac",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gplearn(iobs, fobs, dfobs, x, sig0, lam, ngen, gen):\n",
    "    \"\"\"Sample from the posterior of a Gaussian process given some observed data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    iobs : array\n",
    "        1D array of integers specifying indices i of the x[i] where f(x) has been\n",
    "        observed.\n",
    "    fobs : array\n",
    "        1D array of observed values f(x[i]) for each value i in iobs.\n",
    "    dfobs : array\n",
    "        1D array of Gaussian 1-sigma errors on each observation in fobs.\n",
    "    x : array\n",
    "        1D array of N values of x where each sampled f(x) will be evaluated.\n",
    "    sig0 : float\n",
    "        Gaussian process hyperparameter specifying the vertical scale.\n",
    "    lam : float\n",
    "        Gaussian process hyperparameter specifying the correlation length in x.\n",
    "    ngen : int\n",
    "        Number of functions f(x) to generate with sampling from the posterior.\n",
    "    gen : np.random.RandomState, int or None\n",
    "        Generator or seed to use for reproducible random numbers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        2D array with shape (ngen, N) of ngen sampled functions f(x), each\n",
    "        evaluated at the N input x coordinates.    \n",
    "    \"\"\"\n",
    "    assert len(iobs) == len(fobs) and len(iobs) == len(dfobs)\n",
    "    if not isinstance(gen, np.random.RandomState):\n",
    "        gen = np.random.RandomState(gen)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9cc10f4b11bd43b95e7949660eebfa62",
     "grade": true,
     "grade_id": "cell-43379b0a8c2dd03b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "ngen, sig0, lam = 100, 2.0, 0.5\n",
    "x = np.linspace(-1., 1., 25)\n",
    "gen = np.random.RandomState(seed=123)\n",
    "fx = gplearn([5], [0.], [0.1], x, sig0, lam, ngen, gen)\n",
    "assert fx.shape == (ngen, len(x))\n",
    "assert np.round(np.mean(fx[:, 5]), 1) == 0.0\n",
    "assert np.round(np.std(fx[:, 5]), 1) == 0.1\n",
    "fx = gplearn([20], [1.0], [0.2], x, sig0, lam, ngen, gen)\n",
    "assert fx.shape == (ngen, len(x))\n",
    "assert np.round(np.mean(fx[:, 20]), 1) == 1.0\n",
    "assert np.round(np.std(fx[:, 20]), 1) == 0.2\n",
    "fx = gplearn([5, 20], [0.0, 1.0], [0.1, 0.2], x, sig0, lam, ngen, gen)\n",
    "assert fx.shape == (ngen, len(x))\n",
    "assert np.round(np.mean(fx[:, 5]), 1) == 0.0\n",
    "assert np.round(np.mean(fx[:, 20]), 1) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of posterior samples clearly show how the Gaussian Process prior has been \"filtered\" by the observed data. Note how posterior samples enable us to predict the distribution of $f(x)$ at any **un-observed** value of $x$, and the spread of these predictions grows as we get farther away from the observed values of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7bb5448cb8849c4064e0251a1e85cd64",
     "grade": false,
     "grade_id": "cell-61c2e0a6b03f2f47",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ngen, sig0, lam = 30, 2.0, 0.5\n",
    "x = np.linspace(-1., 1., 100)\n",
    "gen = np.random.RandomState(seed=123)\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(9, 6))\n",
    "fx = gplearn([80], [0.], [0.3], x, sig0, lam, ngen, gen)\n",
    "axes[0,0].plot(x, fx.T, '-')\n",
    "axes[0,0].set_ylabel('$f(x)$')\n",
    "fx = gplearn([80], [1.], [0.3], x, sig0, lam, ngen, gen)\n",
    "axes[0,1].plot(x, fx.T, '-')\n",
    "fx = gplearn([20], [0.], [0.1], x, sig0, lam, ngen, gen)\n",
    "axes[1,0].plot(x, fx.T, '-')\n",
    "axes[1,0].set_xlabel('$x$')\n",
    "axes[1,0].set_ylabel('$f(x)$')\n",
    "fx = gplearn([20, 80], [0., 1.], [0.1, 0.3], x, sig0, lam, ngen, gen)\n",
    "axes[1,1].plot(x, fx.T, '-')\n",
    "axes[1,1].set_xlabel('$x$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about learning with Gaussian Processes, the [definitive reference](http://www.gaussianprocess.org/gpml/) is available as a free ebook.  In many cases, the sampling approach you implemented is not necessary (but still enlightening) since the mean and covariance of the posterior at unobserved points $x_i$ can be calculated directly with some linear algebra. The [scikit-learn GP module](https://scikit-learn.org/stable/modules/gaussian_process.html) is a good starting point for practical calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a probability density $U(x\\mid x_\\text{lo}, x_\\text{hi})$ that is uniform on $x_\\text{lo} \\le x < x_\\text{hi}$.\n",
    "\n",
    "Implement the function below to evaluate the Wasserstein metric $W_1$ between $U(x\\mid x_\\text{lo}, x_\\text{hi})$ and $U(y\\mid y_\\text{lo}, y_\\text{hi})$ using the earlier definition:\n",
    "$$\n",
    "W_1(g, h) = \\int_0^1\\, \\left| G^{-1}(z) - H^{-1}(z)\\right|\\, dz \\; ,\n",
    "$$\n",
    "where $G$ and $H$ are the cumulative distribution functions corresponding to $g$ and $h$. Hint: this integral can be done analytically (but carefully consider the different cases introduced by the absolute value), so your function only needs to evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a00380dbffcc71b26fbb2b4d79b74b41",
     "grade": false,
     "grade_id": "cell-0da7dcb380ce5467",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def W1_uniform(xlo, xhi, ylo, yhi):\n",
    "    \"\"\"Evaluate the Wasserstein distance between two uniform probability densities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xlo, xhi : floats\n",
    "        Range where g(x) = U(x|xlo,xhi) is non zero, with xlo < xhi.\n",
    "    ylo, yhi : floats\n",
    "        Range where h(y) = U(y|ylo,yhi) is non zero, with ylo < yhi.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The distance W1(g, h) calculated using an analytic result.\n",
    "    \"\"\"\n",
    "    assert xlo < xhi and ylo < yhi\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ec0bff4b521df3e940998dbb9865f4c",
     "grade": true,
     "grade_id": "cell-4cb8e4a6eabd19dc",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(W1_uniform(0., 1., 0., 1.), 0)\n",
    "assert np.allclose(W1_uniform(0.5, 1., 0., 1.), 1/4)\n",
    "assert np.allclose(W1_uniform(0., 1., 0., 0.5), 1/4)\n",
    "assert np.allclose(W1_uniform(0., 0.5, 0.5, 1.), 1/2)\n",
    "assert np.allclose(W1_uniform(0.5, 1., 0., 0.5), 1/2)\n",
    "assert np.allclose(W1_uniform(0., 0.5, 1., 1.5), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to calculate the KL divergence between the same two uniform distributions:\n",
    "$$\n",
    "\\text{KL}( g \\parallel h ) \\equiv \\int_{-\\infty}^{+\\infty} d x\\, g(x)\\, \\log\\frac{g(x)}{h(x)}\n",
    "$$\n",
    "As above, you can evaluate this integral analytically, so your function only needs to return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "032c1d0c4f96002a70ba86a743ac8f7c",
     "grade": false,
     "grade_id": "cell-99d398dc7ccd2cb1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL_uniform(xlo, xhi, ylo, yhi):\n",
    "    \"\"\"Evaluate the Kulback-Leibler distance between two uniform probability densities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xlo, xhi : floats\n",
    "        Range where g(x) = U(x|xlo,xhi) is non zero, with xlo < xhi.\n",
    "    ylo, yhi : floats\n",
    "        Range where h(y) = U(y|ylo,yhi) is non zero, with ylo < yhi.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The distance KL(g || h) calculated using an analytic result.\n",
    "    \"\"\"\n",
    "    assert xlo < xhi and ylo < yhi\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18b9f3d58930418ff69500df68b6c1f0",
     "grade": true,
     "grade_id": "cell-01799d17e5f9137f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(KL_uniform(0., 1., 0., 1.), 0)\n",
    "assert np.allclose(KL_uniform(0., 0.5, 0., 1.), np.log(2))\n",
    "assert np.allclose(KL_uniform(0., 1., 0., 0.5), np.inf)\n",
    "assert np.allclose(KL_uniform(0., 0.5, 0.5, 1.), np.inf)\n",
    "assert np.allclose(KL_uniform(0.5, 1., 0., 0.5), np.inf)\n",
    "assert np.allclose(KL_uniform(0., 0.5, 1., 1.5), np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the KL divergence gives the same (infinite) result whenever $h(x) = 0$ is zero somewhere that $g(x) \\ne 0$, so does not provide any useful measure of \"closeness\" in these cases. Contrast this with the earth-mover distance $W_1$ which (roughly) measures how much $g(x)$ would need to be translated to look more like $h(x)$.\n",
    "\n",
    "The plot below fixes the light gray filled distribution and shows the values of W1 and KL relative to a sliding (un-filled) distribution to illustrate this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6be72456a9e45d710666cfd6b00fccf0",
     "grade": false,
     "grade_id": "cell-b8d68a7e507808bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-0.7, 0.7, 500)\n",
    "xlo, xhi = -0.075, 0.075\n",
    "g = scipy.stats.uniform(xlo, xhi - xlo)\n",
    "plt.fill_between(x, g.pdf(x), color='lightgray')\n",
    "for ylo in [-0.6, -0.35, -0.1, 0.15, 0.4]:\n",
    "    yhi = ylo + 0.2\n",
    "    h = scipy.stats.uniform(ylo, yhi - ylo)\n",
    "    L = plt.plot(x, h.pdf(x))\n",
    "    c = L[-1].get_color()\n",
    "    W1 = W1_uniform(xlo, xhi, ylo, yhi)\n",
    "    KL = KL_uniform(xlo, xhi, ylo, yhi)\n",
    "    plt.text(0.5 * (ylo + yhi), 6.0, f'W1={W1:.2f}', color=c, horizontalalignment='center')\n",
    "    plt.text(0.5 * (ylo + yhi), 5.5, f'KL={KL:.2f}', color=c, horizontalalignment='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
